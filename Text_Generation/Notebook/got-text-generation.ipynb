{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-13T15:19:05.899916Z","iopub.execute_input":"2022-11-13T15:19:05.900372Z","iopub.status.idle":"2022-11-13T15:19:05.914775Z","shell.execute_reply.started":"2022-11-13T15:19:05.900333Z","shell.execute_reply":"2022-11-13T15:19:05.913099Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/text-corpus/text_corpus.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow.keras\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import LSTM,Dense,Flatten,Embedding\nfrom keras.preprocessing.sequence import pad_sequences\nimport re\nfrom nltk.corpus import stopwords\nimport string\n\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2022-11-13T16:08:38.387005Z","iopub.execute_input":"2022-11-13T16:08:38.387518Z","iopub.status.idle":"2022-11-13T16:08:38.396160Z","shell.execute_reply.started":"2022-11-13T16:08:38.387479Z","shell.execute_reply":"2022-11-13T16:08:38.394361Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"file=open(\"../input/text-corpus/text_corpus.txt\",'r')\ntext=file.read()\nfile.close()\nprint(text[0:100])","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:16.111315Z","iopub.execute_input":"2022-11-13T15:19:16.112061Z","iopub.status.idle":"2022-11-13T15:19:16.142975Z","shell.execute_reply.started":"2022-11-13T15:19:16.112018Z","shell.execute_reply":"2022-11-13T15:19:16.141521Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"A Game Of Thrones \nBook One of A Song of Ice and Fire \nBy George R. R. Martin \nPROLOGUE \n\"We should \n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_text(text):\n    sample=text\n    sample=re.sub('[%s]'% re.escape(string.punctuation),'',sample)\n    sample=[word for word in sample.split() if word.isalpha()]\n    sample=[word.lower() for word in sample]\n    sample=' '.join(sample)\n    return sample","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:16.145016Z","iopub.execute_input":"2022-11-13T15:19:16.145435Z","iopub.status.idle":"2022-11-13T15:19:16.153700Z","shell.execute_reply.started":"2022-11-13T15:19:16.145400Z","shell.execute_reply":"2022-11-13T15:19:16.152064Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cleaned_text=clean_text(text)\ncleaned_text[0:100]\nprint(len(cleaned_text.split()))","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:16.157905Z","iopub.execute_input":"2022-11-13T15:19:16.158342Z","iopub.status.idle":"2022-11-13T15:19:16.371750Z","shell.execute_reply.started":"2022-11-13T15:19:16.158304Z","shell.execute_reply":"2022-11-13T15:19:16.370568Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"292883\n","output_type":"stream"}]},{"cell_type":"code","source":"seq_doc=[]\nseq_len=50\nl=seq_len+1\ntokens=[w for w in cleaned_text.split()]\nfor i in range(l,len(tokens)):\n    seq=tokens[i-l:i]\n    line=' '.join(seq)\n    seq_doc.append(line)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:16.373059Z","iopub.execute_input":"2022-11-13T15:19:16.373424Z","iopub.status.idle":"2022-11-13T15:19:16.966904Z","shell.execute_reply.started":"2022-11-13T15:19:16.373393Z","shell.execute_reply":"2022-11-13T15:19:16.965546Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer=Tokenizer()\ntokenizer.fit_on_texts(seq_doc)\nsequences=tokenizer.texts_to_sequences(seq_doc)\nvocab_size=len(tokenizer.word_index)+1","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:16.968389Z","iopub.execute_input":"2022-11-13T15:19:16.968841Z","iopub.status.idle":"2022-11-13T15:19:41.512105Z","shell.execute_reply.started":"2022-11-13T15:19:16.968791Z","shell.execute_reply":"2022-11-13T15:19:41.510134Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"sequences=np.array(sequences)\nX,Y=sequences[:,:-1],sequences[:,-1]\nY=to_categorical(Y,num_classes=vocab_size)\nseq_len=X.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:41.514546Z","iopub.execute_input":"2022-11-13T15:19:41.515050Z","iopub.status.idle":"2022-11-13T15:19:44.670378Z","shell.execute_reply.started":"2022-11-13T15:19:41.515009Z","shell.execute_reply":"2022-11-13T15:19:44.668679Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(vocab_size,seq_len)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:44.671996Z","iopub.execute_input":"2022-11-13T15:19:44.672395Z","iopub.status.idle":"2022-11-13T15:19:44.680915Z","shell.execute_reply.started":"2022-11-13T15:19:44.672359Z","shell.execute_reply":"2022-11-13T15:19:44.678802Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"11924 50\n","output_type":"stream"}]},{"cell_type":"code","source":"def model(vocab_size,seq_length):\n    model=Sequential()\n    model.add(Embedding(vocab_size,50,input_length=seq_length))\n    model.add(LSTM(100,return_sequences=True))\n    model.add(LSTM(100))\n    model.add(Dense(100,activation='relu'))\n    model.add(Dense(vocab_size,activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:44.683022Z","iopub.execute_input":"2022-11-13T15:19:44.683475Z","iopub.status.idle":"2022-11-13T15:19:44.694249Z","shell.execute_reply.started":"2022-11-13T15:19:44.683438Z","shell.execute_reply":"2022-11-13T15:19:44.692485Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model=model(11924,50)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:44.695543Z","iopub.execute_input":"2022-11-13T15:19:44.696574Z","iopub.status.idle":"2022-11-13T15:19:45.504207Z","shell.execute_reply.started":"2022-11-13T15:19:44.696530Z","shell.execute_reply":"2022-11-13T15:19:45.502851Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2022-11-13 15:19:44.754425: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 50, 50)            596200    \n_________________________________________________________________\nlstm (LSTM)                  (None, 50, 100)           60400     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               80400     \n_________________________________________________________________\ndense (Dense)                (None, 100)               10100     \n_________________________________________________________________\ndense_1 (Dense)              (None, 11924)             1204324   \n=================================================================\nTotal params: 1,951,424\nTrainable params: 1,951,424\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit(X,Y,batch_size=128,epochs=5)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T15:19:45.506156Z","iopub.execute_input":"2022-11-13T15:19:45.507002Z","iopub.status.idle":"2022-11-13T16:04:00.072189Z","shell.execute_reply.started":"2022-11-13T15:19:45.506952Z","shell.execute_reply":"2022-11-13T16:04:00.070129Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2022-11-13 15:20:23.933103: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n2288/2288 [==============================] - 529s 229ms/step - loss: 6.4792 - accuracy: 0.0688\nEpoch 2/5\n2288/2288 [==============================] - 526s 230ms/step - loss: 5.9337 - accuracy: 0.0954\nEpoch 3/5\n2288/2288 [==============================] - 523s 229ms/step - loss: 5.6369 - accuracy: 0.1141\nEpoch 4/5\n2288/2288 [==============================] - 515s 225ms/step - loss: 5.4261 - accuracy: 0.1238\nEpoch 5/5\n2288/2288 [==============================] - 523s 228ms/step - loss: 5.2674 - accuracy: 0.1306\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f7fb93bc9d0>"},"metadata":{}}]},{"cell_type":"code","source":"model.save('text_gen_model.h5')\njoblib.dump(tokenizer,open('tokenizer_text_gen.joblib','wb'))","metadata":{"execution":{"iopub.status.busy":"2022-11-13T16:13:50.309787Z","iopub.execute_input":"2022-11-13T16:13:50.310329Z","iopub.status.idle":"2022-11-13T16:13:50.866694Z","shell.execute_reply.started":"2022-11-13T16:13:50.310287Z","shell.execute_reply":"2022-11-13T16:13:50.865111Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n    result=list()\n    in_text=seed_text\n    for i in range(n_words):\n        encoded=tokenizer.texts_to_sequences([in_text])[0]\n        encoded=pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n        y=model.predict(encoded,verbose=0)\n        ymax=np.argmax(y,axis=1)\n        out_word=''\n        for word,index in tokenizer.word_index.items():\n            if index==ymax:\n                out_word=word\n                break\n        in_text=in_text+' '+out_word\n        result.append(out_word)\n    return ' '.join(result)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T16:15:58.536900Z","iopub.execute_input":"2022-11-13T16:15:58.537383Z","iopub.status.idle":"2022-11-13T16:15:58.546454Z","shell.execute_reply.started":"2022-11-13T16:15:58.537344Z","shell.execute_reply":"2022-11-13T16:15:58.544961Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"seed_text=seq_doc[np.random.randint(0,len(seq_doc))]\nprint(seed_text+'\\n')\nprint(generate_seq(model,tokenizer,seq_len,seed_text,50))","metadata":{"execution":{"iopub.status.busy":"2022-11-13T16:16:02.047177Z","iopub.execute_input":"2022-11-13T16:16:02.047736Z","iopub.status.idle":"2022-11-13T16:16:06.401382Z","shell.execute_reply.started":"2022-11-13T16:16:02.047692Z","shell.execute_reply":"2022-11-13T16:16:06.399968Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"table he spoke softly yet the high officers of the nights watch all fell quiet the better to hear what the ancient had to say i think he is a giant come among us here at the end of the world tyrion answered gently ive been called many things my lord\n\nthe king said i am a man and the king said i am a man and the king said i am a man and the king said i am a man and the king said i am a man and the king said i am a man and the king\n","output_type":"stream"}]}]}